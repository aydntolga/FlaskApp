{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e87c1e93-050c-449e-93c2-29fb3b331810",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "df = pd.read_excel(\"InventDatasetNew.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f3674cd-2104-4bb7-b658-f5cc0802c9d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\90555\\\\Test'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "287cdfce-af72-4c89-ba51-3932c62f0776",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C:\\\\Users\\\\90555\\\\Test\\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23241fc2-3d3d-410e-acd8-fa46c9e1ca79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Customer</th>\n",
       "      <th>Source</th>\n",
       "      <th>SourceType</th>\n",
       "      <th>Severity</th>\n",
       "      <th>FailType</th>\n",
       "      <th>DagName</th>\n",
       "      <th>FailSummary</th>\n",
       "      <th>Solution</th>\n",
       "      <th>FixTime</th>\n",
       "      <th>Assignee1</th>\n",
       "      <th>Assignee2</th>\n",
       "      <th>YearMonth</th>\n",
       "      <th>Wave</th>\n",
       "      <th>Platform</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-06-26 00:00:00</td>\n",
       "      <td>Fozzy</td>\n",
       "      <td>Product Error</td>\n",
       "      <td>Timeout</td>\n",
       "      <td>Severity-2</td>\n",
       "      <td>daily.wait_reporting</td>\n",
       "      <td>daily</td>\n",
       "      <td>wait_reporting fail düştükten 2dk sonra report...</td>\n",
       "      <td>succsess işaretlendi</td>\n",
       "      <td>00:05:00</td>\n",
       "      <td>Ömer Faruk Başekin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>202306</td>\n",
       "      <td>2023W2</td>\n",
       "      <td>Rocks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-06-30 00:00:00</td>\n",
       "      <td>Fozzy</td>\n",
       "      <td>Product Error</td>\n",
       "      <td>Timeout</td>\n",
       "      <td>Severity-2</td>\n",
       "      <td>noob_daily.fozzy_smartlag_forecast_svp</td>\n",
       "      <td>noob_daily</td>\n",
       "      <td>fozzy_smartlag_forecast_svp taski çok uzun sür...</td>\n",
       "      <td>all purpose clusterdan çalıştırma planlandı. a...</td>\n",
       "      <td>02:00:00</td>\n",
       "      <td>Ömer Faruk Başekin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>202306</td>\n",
       "      <td>2023W2</td>\n",
       "      <td>Rocks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-07-01 00:00:00</td>\n",
       "      <td>Fozzy</td>\n",
       "      <td>Extension Related Error</td>\n",
       "      <td>Extension</td>\n",
       "      <td>Severity-1</td>\n",
       "      <td>rocket_pandas.optimization +2ARAMA</td>\n",
       "      <td>rocket_pandas</td>\n",
       "      <td>Key error hatası alındı.</td>\n",
       "      <td>rabia ile incelendi. sonuç gönderimi yetişmeye...</td>\n",
       "      <td>01:00:00</td>\n",
       "      <td>Esra Miraç Akgün</td>\n",
       "      <td>NaN</td>\n",
       "      <td>202307</td>\n",
       "      <td>2023W2</td>\n",
       "      <td>Rocks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-07-01 00:00:00</td>\n",
       "      <td>Fozzy</td>\n",
       "      <td>Extension Related Error</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Severity-3</td>\n",
       "      <td>fozzy_forecast_output_export.send_rpl_result_test</td>\n",
       "      <td>fozzy_forecast_output_export</td>\n",
       "      <td>önceki hatadan dolayı rocket tamamlanmadığı iç...</td>\n",
       "      <td>Müdehale edilmedi</td>\n",
       "      <td>00:10:00</td>\n",
       "      <td>Esra Miraç Akgün</td>\n",
       "      <td>NaN</td>\n",
       "      <td>202307</td>\n",
       "      <td>2023W2</td>\n",
       "      <td>Rocks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-07-01 00:00:00</td>\n",
       "      <td>Fozzy</td>\n",
       "      <td>Extension Related Error</td>\n",
       "      <td>Retry</td>\n",
       "      <td>Severity-3</td>\n",
       "      <td>fozzy_daily_wasb_to_bq.phantom_stock_to_gc4</td>\n",
       "      <td>fozzy_daily_wasb_to_bq</td>\n",
       "      <td>up for retry</td>\n",
       "      <td>müdahale edilmedi. retry da çalıştı</td>\n",
       "      <td>00:10:00</td>\n",
       "      <td>Esra Miraç Akgün</td>\n",
       "      <td>NaN</td>\n",
       "      <td>202307</td>\n",
       "      <td>2023W2</td>\n",
       "      <td>Rocks</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Date Customer                   Source SourceType  \\\n",
       "0  2023-06-26 00:00:00    Fozzy            Product Error    Timeout   \n",
       "1  2023-06-30 00:00:00    Fozzy            Product Error    Timeout   \n",
       "2  2023-07-01 00:00:00    Fozzy  Extension Related Error  Extension   \n",
       "3  2023-07-01 00:00:00    Fozzy  Extension Related Error        NaN   \n",
       "4  2023-07-01 00:00:00    Fozzy  Extension Related Error      Retry   \n",
       "\n",
       "     Severity                                           FailType  \\\n",
       "0  Severity-2                               daily.wait_reporting   \n",
       "1  Severity-2             noob_daily.fozzy_smartlag_forecast_svp   \n",
       "2  Severity-1                 rocket_pandas.optimization +2ARAMA   \n",
       "3  Severity-3  fozzy_forecast_output_export.send_rpl_result_test   \n",
       "4  Severity-3        fozzy_daily_wasb_to_bq.phantom_stock_to_gc4   \n",
       "\n",
       "                        DagName  \\\n",
       "0                         daily   \n",
       "1                    noob_daily   \n",
       "2                 rocket_pandas   \n",
       "3  fozzy_forecast_output_export   \n",
       "4        fozzy_daily_wasb_to_bq   \n",
       "\n",
       "                                         FailSummary  \\\n",
       "0  wait_reporting fail düştükten 2dk sonra report...   \n",
       "1  fozzy_smartlag_forecast_svp taski çok uzun sür...   \n",
       "2                           Key error hatası alındı.   \n",
       "3  önceki hatadan dolayı rocket tamamlanmadığı iç...   \n",
       "4                                       up for retry   \n",
       "\n",
       "                                            Solution   FixTime  \\\n",
       "0                               succsess işaretlendi  00:05:00   \n",
       "1  all purpose clusterdan çalıştırma planlandı. a...  02:00:00   \n",
       "2  rabia ile incelendi. sonuç gönderimi yetişmeye...  01:00:00   \n",
       "3                                  Müdehale edilmedi  00:10:00   \n",
       "4                müdahale edilmedi. retry da çalıştı  00:10:00   \n",
       "\n",
       "            Assignee1 Assignee2 YearMonth    Wave Platform  \n",
       "0  Ömer Faruk Başekin       NaN    202306  2023W2    Rocks  \n",
       "1  Ömer Faruk Başekin       NaN    202306  2023W2    Rocks  \n",
       "2    Esra Miraç Akgün       NaN    202307  2023W2    Rocks  \n",
       "3    Esra Miraç Akgün       NaN    202307  2023W2    Rocks  \n",
       "4    Esra Miraç Akgün       NaN    202307  2023W2    Rocks  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ede6981-0e68-4e46-baab-fb5c1f940a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.ExcelFile(\"InventDatasetNew.xlsx\") as xls:\n",
    "    df_sheet1 = pd.read_excel(xls, sheet_name=\"Fails\")\n",
    "    df_sheet2 = pd.read_excel(xls, sheet_name=\"Sheet2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f99c29fc-e361-40b7-bbd1-f4781bed21b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Customer</th>\n",
       "      <th>Source</th>\n",
       "      <th>SourceType</th>\n",
       "      <th>Severity</th>\n",
       "      <th>FailType</th>\n",
       "      <th>DagName</th>\n",
       "      <th>FailSummary</th>\n",
       "      <th>Solution</th>\n",
       "      <th>FixTime</th>\n",
       "      <th>Assignee1</th>\n",
       "      <th>Assignee2</th>\n",
       "      <th>YearMonth</th>\n",
       "      <th>Wave</th>\n",
       "      <th>Platform</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>945</th>\n",
       "      <td>4/18/2024</td>\n",
       "      <td>HugoBoss</td>\n",
       "      <td>Infra Instability</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Severity-3</td>\n",
       "      <td>SQLJobFailure - HugoBoss Tableau Refresh Trigger</td>\n",
       "      <td>SQLJobFailure - HugoBoss Tableau Refresh Trigger</td>\n",
       "      <td>tableau refreshlerin de sorun var</td>\n",
       "      <td>tableau server da sorun olunca running kalmış</td>\n",
       "      <td>00:10:00</td>\n",
       "      <td>Ömer Faruk Başekin</td>\n",
       "      <td>Hamza Kılınç</td>\n",
       "      <td>202404</td>\n",
       "      <td>2024W1</td>\n",
       "      <td>Rocks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>946</th>\n",
       "      <td>4/20/2024</td>\n",
       "      <td>HugoBoss</td>\n",
       "      <td>Client Error</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Severity-2</td>\n",
       "      <td>pre_etl.pre_etl_product_list</td>\n",
       "      <td>pre_etl</td>\n",
       "      <td>There are 352 nulls in col product_list_descri...</td>\n",
       "      <td>null olan satırlar drop edilip süreç devam ett...</td>\n",
       "      <td>01:20:00</td>\n",
       "      <td>Ömer Faruk Başekin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>202404</td>\n",
       "      <td>2024W1</td>\n",
       "      <td>Rocks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>947</th>\n",
       "      <td>4/20/2024</td>\n",
       "      <td>HugoBoss</td>\n",
       "      <td>Client Error</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Severity-2</td>\n",
       "      <td>transfer_etl.transfer_constraints</td>\n",
       "      <td>transfer_etl</td>\n",
       "      <td>Cannot insert the value NULL into column 'Arti...</td>\n",
       "      <td>null olan satırlar drop edilip süreç devam ett...</td>\n",
       "      <td>01:00:00</td>\n",
       "      <td>Ömer Faruk Başekin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>202404</td>\n",
       "      <td>2024W1</td>\n",
       "      <td>Rocks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>948</th>\n",
       "      <td>4/20/2024</td>\n",
       "      <td>HugoBoss</td>\n",
       "      <td>Client Error</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Severity-2</td>\n",
       "      <td>pre_etl.pre_etl_article_list</td>\n",
       "      <td>pre_etl</td>\n",
       "      <td>There are 352 nulls in col product_list_descri...</td>\n",
       "      <td>dosya bir önceki günden alındı</td>\n",
       "      <td>00:35:00</td>\n",
       "      <td>Ömer Faruk Başekin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>202404</td>\n",
       "      <td>2024W1</td>\n",
       "      <td>Rocks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>949</th>\n",
       "      <td>4/20/2024</td>\n",
       "      <td>HugoBoss</td>\n",
       "      <td>Client Error</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Severity-2</td>\n",
       "      <td>pre_etl.pre_etl_article_list</td>\n",
       "      <td>pre_etl</td>\n",
       "      <td>There are 352 nulls in col product_list_descri...</td>\n",
       "      <td>dosya bir önceki günden alındı. geçici kontrol</td>\n",
       "      <td>00:30:00</td>\n",
       "      <td>Ömer Faruk Başekin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>202404</td>\n",
       "      <td>2024W1</td>\n",
       "      <td>Rocks</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date  Customer             Source SourceType    Severity  \\\n",
       "945  4/18/2024  HugoBoss  Infra Instability        NaN  Severity-3   \n",
       "946  4/20/2024  HugoBoss       Client Error        NaN  Severity-2   \n",
       "947  4/20/2024  HugoBoss       Client Error        NaN  Severity-2   \n",
       "948  4/20/2024  HugoBoss       Client Error        NaN  Severity-2   \n",
       "949  4/20/2024  HugoBoss       Client Error        NaN  Severity-2   \n",
       "\n",
       "                                             FailType  \\\n",
       "945  SQLJobFailure - HugoBoss Tableau Refresh Trigger   \n",
       "946                      pre_etl.pre_etl_product_list   \n",
       "947                 transfer_etl.transfer_constraints   \n",
       "948                      pre_etl.pre_etl_article_list   \n",
       "949                      pre_etl.pre_etl_article_list   \n",
       "\n",
       "                                              DagName  \\\n",
       "945  SQLJobFailure - HugoBoss Tableau Refresh Trigger   \n",
       "946                                           pre_etl   \n",
       "947                                      transfer_etl   \n",
       "948                                           pre_etl   \n",
       "949                                           pre_etl   \n",
       "\n",
       "                                           FailSummary  \\\n",
       "945                  tableau refreshlerin de sorun var   \n",
       "946  There are 352 nulls in col product_list_descri...   \n",
       "947  Cannot insert the value NULL into column 'Arti...   \n",
       "948  There are 352 nulls in col product_list_descri...   \n",
       "949  There are 352 nulls in col product_list_descri...   \n",
       "\n",
       "                                              Solution   FixTime  \\\n",
       "945      tableau server da sorun olunca running kalmış  00:10:00   \n",
       "946  null olan satırlar drop edilip süreç devam ett...  01:20:00   \n",
       "947  null olan satırlar drop edilip süreç devam ett...  01:00:00   \n",
       "948                     dosya bir önceki günden alındı  00:35:00   \n",
       "949     dosya bir önceki günden alındı. geçici kontrol  00:30:00   \n",
       "\n",
       "              Assignee1     Assignee2 YearMonth    Wave Platform  \n",
       "945  Ömer Faruk Başekin  Hamza Kılınç    202404  2024W1    Rocks  \n",
       "946  Ömer Faruk Başekin           NaN    202404  2024W1    Rocks  \n",
       "947  Ömer Faruk Başekin           NaN    202404  2024W1    Rocks  \n",
       "948  Ömer Faruk Başekin           NaN    202404  2024W1    Rocks  \n",
       "949  Ömer Faruk Başekin           NaN    202404  2024W1    Rocks  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sheet1.tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ff8575e-e398-4287-9c94-e378af8e44f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                 daily.wait_reporting\n",
       "1               noob_daily.fozzy_smartlag_forecast_svp\n",
       "2                   rocket_pandas.optimization +2ARAMA\n",
       "3    fozzy_forecast_output_export.send_rpl_result_test\n",
       "4          fozzy_daily_wasb_to_bq.phantom_stock_to_gc4\n",
       "Name: FailType, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "failTypeRocks = df_sheet1['FailType']\n",
    "failTypeRocks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "221fffb5-91c3-4f87-9d4a-0e4c1b6df22f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Customer</th>\n",
       "      <th>Source</th>\n",
       "      <th>SourceType</th>\n",
       "      <th>Severity</th>\n",
       "      <th>FailType</th>\n",
       "      <th>DagName</th>\n",
       "      <th>FailSummary</th>\n",
       "      <th>Solution</th>\n",
       "      <th>FixTime</th>\n",
       "      <th>...</th>\n",
       "      <th>Wave</th>\n",
       "      <th>Platform</th>\n",
       "      <th>Unnamed: 15</th>\n",
       "      <th>Unnamed: 16</th>\n",
       "      <th>Unnamed: 17</th>\n",
       "      <th>Unnamed: 18</th>\n",
       "      <th>Unnamed: 19</th>\n",
       "      <th>Unnamed: 20</th>\n",
       "      <th>Unnamed: 21</th>\n",
       "      <th>Unnamed: 22</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-01-02 00:00:00</td>\n",
       "      <td>Mavi</td>\n",
       "      <td>Client Error</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mavi SAP Data Transfer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>etl.StagingProducts tablosuna duplike kayıt ek...</td>\n",
       "      <td>saptemp.ProductSeasonHistory tablosunu doldura...</td>\n",
       "      <td>01:30:00</td>\n",
       "      <td>...</td>\n",
       "      <td>2023W1</td>\n",
       "      <td>Maps</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-01-03 00:00:00</td>\n",
       "      <td>Mavi</td>\n",
       "      <td>Client Error</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SQLJobFailure-Mavi SAP Data Transfer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Season dosyası bozuk</td>\n",
       "      <td>Eski günlerdeki dosya ile devam ettirildi.</td>\n",
       "      <td>01:00:00</td>\n",
       "      <td>...</td>\n",
       "      <td>2023W1</td>\n",
       "      <td>Maps</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-01-04 00:00:00</td>\n",
       "      <td>Mavi</td>\n",
       "      <td>Client Error</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SQLJobFailure-Mavi SAP Data Transfer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Season dosyası bozuk</td>\n",
       "      <td>Eski günlerdeki dosya ile devam ettirildi.</td>\n",
       "      <td>01:00:00</td>\n",
       "      <td>...</td>\n",
       "      <td>2023W1</td>\n",
       "      <td>Maps</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-01-08 00:00:00</td>\n",
       "      <td>Mavi</td>\n",
       "      <td>Client Error</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SQLJobFailure-Mavi SAP Data Transfer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sales dosyasında çift kayıt bulunuyordu.</td>\n",
       "      <td>Ecom satışları içinde mağaza satışı yer alması...</td>\n",
       "      <td>01:00:00</td>\n",
       "      <td>...</td>\n",
       "      <td>2023W1</td>\n",
       "      <td>Maps</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-01-24 00:00:00</td>\n",
       "      <td>Mavi</td>\n",
       "      <td>Client Error</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SQLJobFailure-Mavi SAP Data Transfer  The job ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Müşteri malzeme ana verisine eklediğini söyled...</td>\n",
       "      <td>Müşteriye konu ile iligili mail atıldı (Ferhat...</td>\n",
       "      <td>00:45:00</td>\n",
       "      <td>...</td>\n",
       "      <td>2023W1</td>\n",
       "      <td>Maps</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Date Customer        Source SourceType Severity  \\\n",
       "0  2023-01-02 00:00:00     Mavi  Client Error        NaN      NaN   \n",
       "1  2023-01-03 00:00:00     Mavi  Client Error        NaN      NaN   \n",
       "2  2023-01-04 00:00:00     Mavi  Client Error        NaN      NaN   \n",
       "3  2023-01-08 00:00:00     Mavi  Client Error        NaN      NaN   \n",
       "4  2023-01-24 00:00:00     Mavi  Client Error        NaN      NaN   \n",
       "\n",
       "                                            FailType  DagName  \\\n",
       "0                             Mavi SAP Data Transfer      NaN   \n",
       "1               SQLJobFailure-Mavi SAP Data Transfer      NaN   \n",
       "2               SQLJobFailure-Mavi SAP Data Transfer      NaN   \n",
       "3               SQLJobFailure-Mavi SAP Data Transfer      NaN   \n",
       "4  SQLJobFailure-Mavi SAP Data Transfer  The job ...      NaN   \n",
       "\n",
       "                                         FailSummary  \\\n",
       "0  etl.StagingProducts tablosuna duplike kayıt ek...   \n",
       "1                               Season dosyası bozuk   \n",
       "2                               Season dosyası bozuk   \n",
       "3           Sales dosyasında çift kayıt bulunuyordu.   \n",
       "4  Müşteri malzeme ana verisine eklediğini söyled...   \n",
       "\n",
       "                                            Solution   FixTime  ...    Wave  \\\n",
       "0  saptemp.ProductSeasonHistory tablosunu doldura...  01:30:00  ...  2023W1   \n",
       "1         Eski günlerdeki dosya ile devam ettirildi.  01:00:00  ...  2023W1   \n",
       "2         Eski günlerdeki dosya ile devam ettirildi.  01:00:00  ...  2023W1   \n",
       "3  Ecom satışları içinde mağaza satışı yer alması...  01:00:00  ...  2023W1   \n",
       "4  Müşteriye konu ile iligili mail atıldı (Ferhat...  00:45:00  ...  2023W1   \n",
       "\n",
       "  Platform Unnamed: 15 Unnamed: 16 Unnamed: 17  Unnamed: 18  Unnamed: 19  \\\n",
       "0     Maps         NaN         NaN         NaN          NaN          NaN   \n",
       "1     Maps         NaN         NaN         NaN          NaN          NaN   \n",
       "2     Maps         NaN         NaN         NaN          NaN          NaN   \n",
       "3     Maps         NaN         NaN         NaN          NaN          NaN   \n",
       "4     Maps         NaN         NaN         NaN          NaN          NaN   \n",
       "\n",
       "   Unnamed: 20  Unnamed: 21  Unnamed: 22  \n",
       "0          NaN          NaN          NaN  \n",
       "1          NaN          NaN          NaN  \n",
       "2          NaN          NaN          NaN  \n",
       "3          NaN          NaN          NaN  \n",
       "4          NaN          NaN          NaN  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sheet2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51f03fce-4439-4120-a0d3-afd78be39751",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import joblib\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "def preprocess_text(text):\n",
    "    if isinstance(text, str):  \n",
    "        text = text.lower()\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "        text = text.strip()\n",
    "        stop_words = set(stopwords.words('turkish'))\n",
    "        word_tokens = word_tokenize(text)\n",
    "        filtered_text = [word for word in word_tokens if word not in stop_words]\n",
    "        \n",
    "        stemmer = PorterStemmer()\n",
    "        stemmed_text = [stemmer.stem(word) for word in filtered_text]\n",
    "       \n",
    "        preprocessed_text = ' '.join(stemmed_text)\n",
    "        return preprocessed_text\n",
    "    else:  \n",
    "        return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c1566ecb-a22d-4088-a564-fe937fc10abb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;preprocessor&#x27;,\n",
       "                 ColumnTransformer(remainder=&#x27;passthrough&#x27;,\n",
       "                                   transformers=[(&#x27;encoder&#x27;,\n",
       "                                                  OneHotEncoder(handle_unknown=&#x27;ignore&#x27;),\n",
       "                                                  [&#x27;Customer&#x27;, &#x27;FailType&#x27;]),\n",
       "                                                 (&#x27;vectorizer&#x27;,\n",
       "                                                  TfidfVectorizer(preprocessor=&lt;function preprocess_text at 0x00000289DE09A980&gt;),\n",
       "                                                  &#x27;FailSummary&#x27;),\n",
       "                                                 (&#x27;vectorizer_sourceType&#x27;,\n",
       "                                                  TfidfVectorizer(preprocessor=&lt;function preprocess_text at 0x00000289DE09A980&gt;),\n",
       "                                                  &#x27;SourceType&#x27;),\n",
       "                                                 (&#x27;vectorizer_source&#x27;,\n",
       "                                                  TfidfVectorizer(preprocessor=&lt;function preprocess_text at 0x00000289DE09A980&gt;),\n",
       "                                                  &#x27;Source&#x27;)])),\n",
       "                (&#x27;classifier&#x27;, SVC(kernel=&#x27;linear&#x27;))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-14\" type=\"checkbox\" ><label for=\"sk-estimator-id-14\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;preprocessor&#x27;,\n",
       "                 ColumnTransformer(remainder=&#x27;passthrough&#x27;,\n",
       "                                   transformers=[(&#x27;encoder&#x27;,\n",
       "                                                  OneHotEncoder(handle_unknown=&#x27;ignore&#x27;),\n",
       "                                                  [&#x27;Customer&#x27;, &#x27;FailType&#x27;]),\n",
       "                                                 (&#x27;vectorizer&#x27;,\n",
       "                                                  TfidfVectorizer(preprocessor=&lt;function preprocess_text at 0x00000289DE09A980&gt;),\n",
       "                                                  &#x27;FailSummary&#x27;),\n",
       "                                                 (&#x27;vectorizer_sourceType&#x27;,\n",
       "                                                  TfidfVectorizer(preprocessor=&lt;function preprocess_text at 0x00000289DE09A980&gt;),\n",
       "                                                  &#x27;SourceType&#x27;),\n",
       "                                                 (&#x27;vectorizer_source&#x27;,\n",
       "                                                  TfidfVectorizer(preprocessor=&lt;function preprocess_text at 0x00000289DE09A980&gt;),\n",
       "                                                  &#x27;Source&#x27;)])),\n",
       "                (&#x27;classifier&#x27;, SVC(kernel=&#x27;linear&#x27;))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-15\" type=\"checkbox\" ><label for=\"sk-estimator-id-15\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">preprocessor: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(remainder=&#x27;passthrough&#x27;,\n",
       "                  transformers=[(&#x27;encoder&#x27;,\n",
       "                                 OneHotEncoder(handle_unknown=&#x27;ignore&#x27;),\n",
       "                                 [&#x27;Customer&#x27;, &#x27;FailType&#x27;]),\n",
       "                                (&#x27;vectorizer&#x27;,\n",
       "                                 TfidfVectorizer(preprocessor=&lt;function preprocess_text at 0x00000289DE09A980&gt;),\n",
       "                                 &#x27;FailSummary&#x27;),\n",
       "                                (&#x27;vectorizer_sourceType&#x27;,\n",
       "                                 TfidfVectorizer(preprocessor=&lt;function preprocess_text at 0x00000289DE09A980&gt;),\n",
       "                                 &#x27;SourceType&#x27;),\n",
       "                                (&#x27;vectorizer_source&#x27;,\n",
       "                                 TfidfVectorizer(preprocessor=&lt;function preprocess_text at 0x00000289DE09A980&gt;),\n",
       "                                 &#x27;Source&#x27;)])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-16\" type=\"checkbox\" ><label for=\"sk-estimator-id-16\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">encoder</label><div class=\"sk-toggleable__content\"><pre>[&#x27;Customer&#x27;, &#x27;FailType&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-17\" type=\"checkbox\" ><label for=\"sk-estimator-id-17\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">OneHotEncoder</label><div class=\"sk-toggleable__content\"><pre>OneHotEncoder(handle_unknown=&#x27;ignore&#x27;)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-18\" type=\"checkbox\" ><label for=\"sk-estimator-id-18\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">vectorizer</label><div class=\"sk-toggleable__content\"><pre>FailSummary</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-19\" type=\"checkbox\" ><label for=\"sk-estimator-id-19\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(preprocessor=&lt;function preprocess_text at 0x00000289DE09A980&gt;)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-20\" type=\"checkbox\" ><label for=\"sk-estimator-id-20\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">vectorizer_sourceType</label><div class=\"sk-toggleable__content\"><pre>SourceType</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-21\" type=\"checkbox\" ><label for=\"sk-estimator-id-21\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(preprocessor=&lt;function preprocess_text at 0x00000289DE09A980&gt;)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-22\" type=\"checkbox\" ><label for=\"sk-estimator-id-22\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">vectorizer_source</label><div class=\"sk-toggleable__content\"><pre>Source</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-23\" type=\"checkbox\" ><label for=\"sk-estimator-id-23\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(preprocessor=&lt;function preprocess_text at 0x00000289DE09A980&gt;)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-24\" type=\"checkbox\" ><label for=\"sk-estimator-id-24\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">remainder</label><div class=\"sk-toggleable__content\"><pre>[&#x27;DagName&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-25\" type=\"checkbox\" ><label for=\"sk-estimator-id-25\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">passthrough</label><div class=\"sk-toggleable__content\"><pre>passthrough</pre></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-26\" type=\"checkbox\" ><label for=\"sk-estimator-id-26\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(kernel=&#x27;linear&#x27;)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('preprocessor',\n",
       "                 ColumnTransformer(remainder='passthrough',\n",
       "                                   transformers=[('encoder',\n",
       "                                                  OneHotEncoder(handle_unknown='ignore'),\n",
       "                                                  ['Customer', 'FailType']),\n",
       "                                                 ('vectorizer',\n",
       "                                                  TfidfVectorizer(preprocessor=<function preprocess_text at 0x00000289DE09A980>),\n",
       "                                                  'FailSummary'),\n",
       "                                                 ('vectorizer_sourceType',\n",
       "                                                  TfidfVectorizer(preprocessor=<function preprocess_text at 0x00000289DE09A980>),\n",
       "                                                  'SourceType'),\n",
       "                                                 ('vectorizer_source',\n",
       "                                                  TfidfVectorizer(preprocessor=<function preprocess_text at 0x00000289DE09A980>),\n",
       "                                                  'Source')])),\n",
       "                ('classifier', SVC(kernel='linear'))])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier,export_graphviz\n",
    "from sklearn.model_selection import train_test_split,cross_val_score\n",
    "\n",
    "# Verilerin temizlenmesi\n",
    "df_sheet2['FailSummary'] = df_sheet2['FailSummary'].apply(preprocess_text)\n",
    "df_sheet2['FailType'] = df_sheet2['FailType'].apply(preprocess_text)\n",
    "df_sheet2['Customer'] = df_sheet2['Customer'].apply(preprocess_text)\n",
    "df_sheet2['DagName'] = df_sheet2['DagName'].apply(preprocess_text)\n",
    "df_sheet2['Source'] = df_sheet2['Source'].apply(preprocess_text)\n",
    "df_sheet2['SourceType'] = df_sheet2['SourceType'].apply(preprocess_text)\n",
    "df_sheet2['Solution'] = df_sheet2['Solution'].apply(preprocess_text)\n",
    "\n",
    "df_sheet2.fillna(0, inplace=True)\n",
    "\n",
    "\n",
    "df_sheet2 = df_sheet2.astype(str)\n",
    "\n",
    "X = df_sheet2[['Customer','Source','SourceType', 'FailType', 'FailSummary','DagName']]\n",
    "y = df_sheet2['Solution']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "column_transformer = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('encoder', OneHotEncoder(handle_unknown='ignore'), ['Customer', 'FailType']),       \n",
    "        ('vectorizer', TfidfVectorizer(preprocessor=preprocess_text), 'FailSummary'),\n",
    "        ('vectorizer_sourceType', TfidfVectorizer(preprocessor=preprocess_text), 'SourceType'),\n",
    "        #('vectorizer_dag', TfidfVectorizer(preprocessor=preprocess_text),'DagName'),\n",
    "        ('vectorizer_source', TfidfVectorizer(preprocessor=preprocess_text), 'Source')\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "pipelineMaps = Pipeline([\n",
    "    ('preprocessor', column_transformer),\n",
    "    ('classifier', SVC(kernel='linear'))\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "pipelineMaps.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3fa5bd9e-3f7f-4658-b10d-83b692289ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy: 0.8281938325991189\n"
     ]
    }
   ],
   "source": [
    "accuracy = pipelineMaps.score(X_test, y_test)\n",
    "print(\"Model accuracy:\", accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f42bea98-235d-4f7b-a4e0-ae860c6786a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: joblib in d:\\anaconda\\lib\\site-packages (1.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7c109d25-0e8c-4559-a67f-486feae1b7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump\n",
    "\n",
    "\n",
    "dump(pipelineMaps, 'modelMaps.pkl')\n",
    "from joblib import load\n",
    "\n",
    "\n",
    "loaded_model = load('modelMaps.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4fc5cf6b-1fed-4cd9-b330-d32d6f0fda3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('modelMaps.pkl','rb') as file:\n",
    "    object = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19778c80-732d-47c9-9b9b-fa629548afc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting flask-restful\n",
      "  Downloading Flask_RESTful-0.3.10-py2.py3-none-any.whl.metadata (1.0 kB)\n",
      "Collecting aniso8601>=0.82 (from flask-restful)\n",
      "  Downloading aniso8601-9.0.1-py2.py3-none-any.whl.metadata (23 kB)\n",
      "Requirement already satisfied: Flask>=0.8 in d:\\anaconda\\lib\\site-packages (from flask-restful) (2.2.5)\n",
      "Requirement already satisfied: six>=1.3.0 in d:\\anaconda\\lib\\site-packages (from flask-restful) (1.16.0)\n",
      "Requirement already satisfied: pytz in d:\\anaconda\\lib\\site-packages (from flask-restful) (2023.3.post1)\n",
      "Requirement already satisfied: Werkzeug>=2.2.2 in d:\\anaconda\\lib\\site-packages (from Flask>=0.8->flask-restful) (2.2.3)\n",
      "Requirement already satisfied: Jinja2>=3.0 in d:\\anaconda\\lib\\site-packages (from Flask>=0.8->flask-restful) (3.1.3)\n",
      "Requirement already satisfied: itsdangerous>=2.0 in d:\\anaconda\\lib\\site-packages (from Flask>=0.8->flask-restful) (2.0.1)\n",
      "Requirement already satisfied: click>=8.0 in d:\\anaconda\\lib\\site-packages (from Flask>=0.8->flask-restful) (8.1.7)\n",
      "Requirement already satisfied: colorama in d:\\anaconda\\lib\\site-packages (from click>=8.0->Flask>=0.8->flask-restful) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\anaconda\\lib\\site-packages (from Jinja2>=3.0->Flask>=0.8->flask-restful) (2.1.3)\n",
      "Downloading Flask_RESTful-0.3.10-py2.py3-none-any.whl (26 kB)\n",
      "Downloading aniso8601-9.0.1-py2.py3-none-any.whl (52 kB)\n",
      "   ---------------------------------------- 0.0/52.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 52.8/52.8 kB 1.4 MB/s eta 0:00:00\n",
      "Installing collected packages: aniso8601, flask-restful\n",
      "Successfully installed aniso8601-9.0.1 flask-restful-0.3.10\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install flask-restful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b263b22f-e042-4de8-bdd0-66549ee04032",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import jsonify,Flask,request\n",
    "from flask_restful import Api,Resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "046f3682-4e60-49e2-a6e2-006915059b86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      " * Restarting with watchdog (windowsapi)\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3561: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "app = Flask(__name__)\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    # Gelen verileri al\n",
    "    data = request.json\n",
    "    \n",
    "    # Modeli kullanarak tahmin yap\n",
    "    prediction = model.predict(data['features'])\n",
    "    \n",
    "    # Tahmini JSON olarak geri dön\n",
    "    return jsonify({'prediction': prediction.tolist()})\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e89d00dc-e222-4af0-8adf-a01f6b8808fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import joblib\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "def preprocess_text(text):\n",
    "    if isinstance(text, str):  # Metin tipinde ise ön işlem yap\n",
    "        # Küçük harfe dönüştürme\n",
    "        text = text.lower()\n",
    "        # Noktalama işaretlerini kaldırma\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "        # Boşlukları kaldırma\n",
    "        text = text.strip()\n",
    "        # Stopwords'leri kaldırma\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        word_tokens = word_tokenize(text)\n",
    "        filtered_text = [word for word in word_tokens if word not in stop_words]\n",
    "        # Kelime köklerini alma (stemming)\n",
    "        stemmer = PorterStemmer()\n",
    "        stemmed_text = [stemmer.stem(word) for word in filtered_text]\n",
    "        # Tekrar oluşturulmuş metni birleştirme\n",
    "        preprocessed_text = ' '.join(stemmed_text)\n",
    "        return preprocessed_text\n",
    "    else:  # Metin tipinde değilse, orijinal değeri geri döndür\n",
    "        return text\n",
    "def preprocess_text_column(column):\n",
    "    return column.apply(preprocess_text).fillna(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3af2e5aa-5b1e-49af-abfe-fc521306cd3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>OneHotEncoder()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-14\" type=\"checkbox\" checked><label for=\"sk-estimator-id-14\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">OneHotEncoder</label><div class=\"sk-toggleable__content\"><pre>OneHotEncoder()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "OneHotEncoder()"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.sparse import vstack\n",
    "import pandas as pd\n",
    "\n",
    "# Verilerin temizlenmesi\n",
    "df_sheet1['FailSummary'] = df_sheet1['FailSummary'].apply(preprocess_text)\n",
    "df_sheet1['FailType'] = df_sheet1['FailType'].apply(preprocess_text)\n",
    "df_sheet1['Customer'] = df_sheet1['Customer'].apply(preprocess_text)\n",
    "df_sheet1['DagName'] = df_sheet1['DagName'].apply(preprocess_text)\n",
    "df_sheet1['Source'] = df_sheet1['Source'].apply(preprocess_text)\n",
    "df_sheet1['SourceType'] = df_sheet1['SourceType'].apply(preprocess_text)\n",
    "df_sheet1['Solution'] = df_sheet1['Solution'].apply(preprocess_text)\n",
    "\n",
    "df_sheet1.fillna(,inplace=True)\n",
    "\n",
    "X = df_sheet1[['Customer', 'Source', 'SourceType', 'DagName', 'FailType', 'FailSummary']]\n",
    "y = df_sheet1['Solution']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=442)\n",
    "\n",
    "# Kategorik sütunları seçin\n",
    "categorical_columns = ['Customer', 'Source', 'SourceType', 'DagName', 'FailType']\n",
    "\n",
    "# Tüm kategorik değerleri toplamak için OneHotEncoder oluşturun\n",
    "encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "for column in categorical_columns:\n",
    "  unique_values_train = X_train[column].unique()\n",
    "  unique_values_test = X_test[column].unique()\n",
    "  all_unique_values = set(unique_values_train).union(set(unique_values_test))\n",
    "  encoder.fit([[value] for value in all_unique_values])\n",
    "\n",
    "# TfidfVectorizer'ı fit etme\n",
    "vectorizer_failsummary = TfidfVectorizer()\n",
    "vectorizer_failsummary.fit(X_train['FailSummary'])\n",
    "\n",
    "\n",
    "vectorizer_customer = TfidfVectorizer()\n",
    "vectorizer_customer.fit(X_train['Customer'])\n",
    "\n",
    "\n",
    "onehotencoder_source = OneHotEncoder()\n",
    "onehotencoder_source.fit(X_train['Source'].values.reshape(-1, 1))\n",
    "\n",
    "onehotencoder_sourceType = OneHotEncoder()\n",
    "onehotencoder_sourceType.fit(X_train['SourceType'].values.reshape(-1, 1))\n",
    "\n",
    "onehotencoder_dag = OneHotEncoder()\n",
    "onehotencoder_dag.fit(X_train['DagName'].values.reshape(-1, 1))\n",
    "\n",
    "onehotencoder_failtype = OneHotEncoder()\n",
    "onehotencoder_failtype.fit(X_train['FailType'].values.reshape(-1, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "159ccc3d-c212-463e-a693-d96d2c7b8048",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Specifying the columns using strings is only supported for pandas DataFrames",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\sklearn\\utils\\__init__.py:424\u001b[0m, in \u001b[0;36m_get_column_indices\u001b[1;34m(X, key)\u001b[0m\n\u001b[0;32m    423\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 424\u001b[0m     all_columns \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[0;32m    425\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'csr_matrix' object has no attribute 'columns'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 24\u001b[0m\n\u001b[0;32m     19\u001b[0m X_train_transformed \u001b[38;5;241m=\u001b[39m column_transformer\u001b[38;5;241m.\u001b[39mfit_transform(X_train)\n\u001b[0;32m     20\u001b[0m X_test_transformed \u001b[38;5;241m=\u001b[39m column_transformer\u001b[38;5;241m.\u001b[39mtransform(X_test)\n\u001b[1;32m---> 24\u001b[0m pipeline_rocks\u001b[38;5;241m.\u001b[39mfit(X_train_transformed, y_train)\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\sklearn\\pipeline.py:401\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fit the model.\u001b[39;00m\n\u001b[0;32m    376\u001b[0m \n\u001b[0;32m    377\u001b[0m \u001b[38;5;124;03mFit all the transformers one after the other and transform the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    398\u001b[0m \u001b[38;5;124;03m    Pipeline with fitted steps.\u001b[39;00m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    400\u001b[0m fit_params_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_fit_params(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[1;32m--> 401\u001b[0m Xt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params_steps)\n\u001b[0;32m    402\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_message(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)):\n\u001b[0;32m    403\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\sklearn\\pipeline.py:359\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[1;34m(self, X, y, **fit_params_steps)\u001b[0m\n\u001b[0;32m    357\u001b[0m     cloned_transformer \u001b[38;5;241m=\u001b[39m clone(transformer)\n\u001b[0;32m    358\u001b[0m \u001b[38;5;66;03m# Fit or load from cache the current transformer\u001b[39;00m\n\u001b[1;32m--> 359\u001b[0m X, fitted_transformer \u001b[38;5;241m=\u001b[39m fit_transform_one_cached(\n\u001b[0;32m    360\u001b[0m     cloned_transformer,\n\u001b[0;32m    361\u001b[0m     X,\n\u001b[0;32m    362\u001b[0m     y,\n\u001b[0;32m    363\u001b[0m     \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    364\u001b[0m     message_clsname\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    365\u001b[0m     message\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_message(step_idx),\n\u001b[0;32m    366\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params_steps[name],\n\u001b[0;32m    367\u001b[0m )\n\u001b[0;32m    368\u001b[0m \u001b[38;5;66;03m# Replace the transformer of the step with the fitted\u001b[39;00m\n\u001b[0;32m    369\u001b[0m \u001b[38;5;66;03m# transformer. This is necessary when loading the transformer\u001b[39;00m\n\u001b[0;32m    370\u001b[0m \u001b[38;5;66;03m# from the cache.\u001b[39;00m\n\u001b[0;32m    371\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[step_idx] \u001b[38;5;241m=\u001b[39m (name, fitted_transformer)\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\joblib\\memory.py:349\u001b[0m, in \u001b[0;36mNotMemorizedFunc.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 349\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\sklearn\\pipeline.py:893\u001b[0m, in \u001b[0;36m_fit_transform_one\u001b[1;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[0;32m    891\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[0;32m    892\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(transformer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 893\u001b[0m         res \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit_transform(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m    894\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    895\u001b[0m         res \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 140\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    142\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    143\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    145\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    146\u001b[0m         )\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py:724\u001b[0m, in \u001b[0;36mColumnTransformer.fit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    722\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_n_features(X, reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    723\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_transformers()\n\u001b[1;32m--> 724\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_column_callables(X)\n\u001b[0;32m    725\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_remainder(X)\n\u001b[0;32m    727\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_transform(X, y, _fit_transform_one)\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py:426\u001b[0m, in \u001b[0;36mColumnTransformer._validate_column_callables\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    424\u001b[0m         columns \u001b[38;5;241m=\u001b[39m columns(X)\n\u001b[0;32m    425\u001b[0m     all_columns\u001b[38;5;241m.\u001b[39mappend(columns)\n\u001b[1;32m--> 426\u001b[0m     transformer_to_input_indices[name] \u001b[38;5;241m=\u001b[39m _get_column_indices(X, columns)\n\u001b[0;32m    428\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_columns \u001b[38;5;241m=\u001b[39m all_columns\n\u001b[0;32m    429\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transformer_to_input_indices \u001b[38;5;241m=\u001b[39m transformer_to_input_indices\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\sklearn\\utils\\__init__.py:426\u001b[0m, in \u001b[0;36m_get_column_indices\u001b[1;34m(X, key)\u001b[0m\n\u001b[0;32m    424\u001b[0m     all_columns \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[0;32m    425\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[1;32m--> 426\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    427\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpecifying the columns using strings is only \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    428\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msupported for pandas DataFrames\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    429\u001b[0m     )\n\u001b[0;32m    430\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    431\u001b[0m     columns \u001b[38;5;241m=\u001b[39m [key]\n",
      "\u001b[1;31mValueError\u001b[0m: Specifying the columns using strings is only supported for pandas DataFrames"
     ]
    }
   ],
   "source": [
    "# ColumnTransformer oluşturun\n",
    "column_transformer = ColumnTransformer(\n",
    "  transformers=[\n",
    "    ('vectorizer_customer', TfidfVectorizer(), 'Customer'),\n",
    "    ('onehot_encoder', OneHotEncoder(handle_unknown='ignore'), ['Source', 'SourceType', 'DagName', 'FailType']),\n",
    "    ('vectorizer_failsummary', TfidfVectorizer(), 'FailSummary')  # TF-IDF for FailSummary\n",
    "  ],\n",
    "  remainder='passthrough'\n",
    ")\n",
    "\n",
    "\n",
    "# Pipeline oluşturma\n",
    "pipeline_rocks = Pipeline([\n",
    "  ('preprocessor', column_transformer),\n",
    "  ('classifier', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "# Modelin eğitimi\n",
    "X_train_transformed = column_transformer.fit_transform(X_train)\n",
    "X_test_transformed = column_transformer.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "pipeline_rocks.fit(X_train_transformed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ef6ee5-a8ab-43dc-86a3-b9b367d3c5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Test verileri üzerinde tahmin yapma\n",
    "y_pred = pipeline_rocks.predict(X_test)\n",
    "\n",
    "\n",
    "# Doğruluk hesaplama\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afdcd2e-12d6-410e-b73b-4657a287cb28",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.sparse import vstack\n",
    "import pandas as pd\n",
    "\n",
    "# Verilerin temizlenmesi\n",
    "df_sheet1['FailSummary'] = df_sheet1['FailSummary'].apply(preprocess_text)\n",
    "df_sheet1['FailType'] = df_sheet1['FailType'].apply(preprocess_text)\n",
    "df_sheet1['Customer'] = df_sheet1['Customer'].apply(preprocess_text)\n",
    "df_sheet1['DagName'] = df_sheet1['DagName'].apply(preprocess_text)\n",
    "df_sheet1['Source'] = df_sheet1['Source'].apply(preprocess_text)\n",
    "df_sheet1['SourceType'] = df_sheet1['SourceType'].apply(preprocess_text)\n",
    "df_sheet1['Solution'] = df_sheet1['Solution'].apply(preprocess_text)\n",
    "\n",
    "\n",
    "# Kategorik sütunları seçin\n",
    "categorical_columns = ['Customer', 'FailType','Source','SourceType']\n",
    "\n",
    "# LabelEncoder kullanarak kategorik sütunları dönüştürün\n",
    "label_encoders = {}\n",
    "for column in categorical_columns:\n",
    "    label_encoders[column] = LabelEncoder()\n",
    "    X_train[column] = label_encoders[column].fit_transform(X_train[column])\n",
    "\n",
    "# Test verilerinde görülmeyen etiketleri kontrol edin\n",
    "for column in categorical_columns:\n",
    "    unseen_labels = set(X_test[column]) - set(label_encoders[column].classes_)\n",
    "    if unseen_labels:\n",
    "        print(f\"Test verilerinde görülmeyen etiketler: {unseen_labels}\")\n",
    "\n",
    "\n",
    "# Test verilerindeki eşsiz etiketleri eğitim verilerine ekleyin\n",
    "for column in categorical_columns:\n",
    "    all_labels = sorted(set(list(label_encoders[column].classes_) + list(X_test[column])))\n",
    "    label_encoders[column].classes_ = all_labels\n",
    "\n",
    "for column in categorical_columns:\n",
    "    X_test[column] = X_test[column].astype(str)\n",
    "\n",
    "# Test verilerine LabelEncoder'ı uygulayın\n",
    "for column in categorical_columns:\n",
    "    X_test[column] = label_encoders[column].transform(X_test[column])\n",
    "for column in categorical_columns:\n",
    "    unseen_labels = set(X_test[column]) - set(label_encoders[column].classes_)\n",
    "    print(unseen_labels)\n",
    "    if unseen_labels:\n",
    "        print(f\"Test verilerinde görülmeyen etiketler: {unseen_labels}\")\n",
    "\n",
    "\n",
    "# Sütun dönüşümünü tanımlama\n",
    "column_transformer = ColumnTransformer(\n",
    "    [('encoder', OneHotEncoder(), ['Customer', 'FailType']),],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "X['FailSummary'].fillna('', inplace=True)\n",
    "\n",
    "# Pipeline oluşturma\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', column_transformer),\n",
    "    ('classifier', SVC(kernel='linear'))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc319b1-df90-4594-a79d-02d77f3a40da",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_indices = X['FailSummary'].isnull()\n",
    "nan_values = X.loc[nan_indices, 'FailSummary']\n",
    "print(nan_values)\n",
    "X['FailSummary'].replace('', np.nan, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5ea99e-2b6b-4d8c-b4b6-28fde58cb947",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4137484d-5def-4af3-80e6-b90571725ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "failTypeMaps = df_sheet2['FailType']\n",
    "failTypeMaps.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b451ac-3b07-442f-8389-ffed3419206e",
   "metadata": {},
   "outputs": [],
   "source": [
    "failTypeMaps = failTypeMaps.str.lower()\n",
    "failTypeMaps.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a0b674-a6cf-41e8-9f6d-d04167ca87fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Preprocess_text fonksiyonunu tanımla\n",
    "def preprocess_text(text):\n",
    "    if isinstance(text, str):  # Metin tipinde ise ön işlem yap\n",
    "        # Küçük harfe dönüştürme\n",
    "        text = text.lower()\n",
    "        # Noktalama işaretlerini kaldırma\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "        # Boşlukları kaldırma\n",
    "        text = text.strip()\n",
    "        # Stopwords'leri kaldırma\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        word_tokens = word_tokenize(text)\n",
    "        filtered_text = [word for word in word_tokens if word not in stop_words]\n",
    "        # Kelime köklerini alma (stemming)\n",
    "        stemmer = PorterStemmer()\n",
    "        stemmed_text = [stemmer.stem(word) for word in filtered_text]\n",
    "        # Tekrar oluşturulmuş metni birleştirme\n",
    "        preprocessed_text = ' '.join(stemmed_text)\n",
    "        return preprocessed_text\n",
    "    else:  # Metin tipinde değilse, orijinal değeri geri döndür\n",
    "        return text\n",
    "\n",
    "# Ön işlem için metin sütununu ön işleme fonksiyonundan geçirme\n",
    "X['FailSummary'] = X['FailSummary'].apply(preprocess_text)\n",
    "\n",
    "# Pipeline içinde TfidfVectorizer'ı ve SimpleImputer'ı bir araya getirme\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('tfidf', TfidfVectorizer(), 'FailSummary'),\n",
    "        ('imputer', SimpleImputer(strategy='constant', fill_value=0), ['FailSummary'])\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Pipeline oluşturma\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor)\n",
    "])\n",
    "\n",
    "# NaN değerleri çıkar\n",
    "X_train.dropna(subset=['FailSummary'], inplace=True)\n",
    "\n",
    "\n",
    "# Pipeline'i eğitme\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Metin sütununu ön işleme fonksiyonundan geçirme\n",
    "#X['FailSummary'] = X['FailSummary'].apply(preprocess_text)\n",
    "\n",
    "# TfidfVectorizer'ı oluşturma\n",
    "#tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# NaN değerlerini boş bir metin ile doldur\n",
    "#X['FailSummary'].fillna('', inplace=True)\n",
    "\n",
    "# Pipeline'i eğitme\n",
    "#pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Eğitim veri setinde modelin doğruluğunu değerlendirme\n",
    "#train_accuracy = pipeline.score(X_train, y_train)\n",
    "#print(\"Eğitim veri seti doğruluğu:\", train_accuracy)\n",
    "\n",
    "# Test veri setinde modelin doğruluğunu değerlendirme\n",
    "#test_accuracy = pipeline.score(X_test, y_test)\n",
    "#print(\"Test veri seti doğruluğu:\", test_accuracy)\n",
    "\n",
    "# Modeli diske kaydetme\n",
    "#joblib.dump(pipeline, 'solution_model.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1592b6d5-4b08-4e3d-86df-340cf860b222",
   "metadata": {},
   "outputs": [],
   "source": [
    "failTypeMaps = failTypeMaps.replace('[^\\w\\s]', ' ')\n",
    "failTypeMaps.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf13e3e-4444-4e94-82c0-2b353cc7ee22",
   "metadata": {},
   "outputs": [],
   "source": [
    "failTypeMaps = failTypeMaps.replace('\\s+', ' ')\n",
    "failTypeMaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b82445-91fb-400a-8b52-e104c2655426",
   "metadata": {},
   "outputs": [],
   "source": [
    "failTypeRocks = failTypeRocks.str.lower()\n",
    "failTypeRocks = failTypeRocks.str.replace('[^\\w\\s]', ' ')\n",
    "failTypeRocks = failTypeRocks.str.replace('\\s+', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11b6a33-f208-41e4-8399-2c976d61ec12",
   "metadata": {},
   "outputs": [],
   "source": [
    "failTypeRocks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffcd379-766d-476c-bbe1-0eaeb30daddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "failTypeCountsMaps = df_sheet2['FailType'].value_counts()\n",
    "\n",
    "# Her bir FailType'in adını ve sayısını içeren bir sözlük oluşturma\n",
    "failTypeCountsMaps_dict = {failType: count for failType, count in failTypeCountsMaps.items()}\n",
    "\n",
    "# Oluşturulan sözlüğü bir değişkene atama\n",
    "failTypeCountsMaps_variable = failTypeCountsMaps_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a3d681-d8db-461d-bda3-2e9079345882",
   "metadata": {},
   "outputs": [],
   "source": [
    "failTypeCountsMaps.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edaaa657-79d2-4d12-b7de-3d0af5b1798f",
   "metadata": {},
   "outputs": [],
   "source": [
    "failTypeCountsMaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d62e900-fea8-4d93-b1ae-fb4ba4774146",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "def text_process(failTypeMaps):\n",
    "    # Boş veya eksik değerleri temizle\n",
    "    failTypeMaps = failTypeMaps.fillna(\"\")\n",
    "\n",
    "    # Kök bulma işlemi için stemmer oluştur\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    # Metin önişleme adımlarını uygul\n",
    "    failTypeMaps = failTypeMaps.apply(lambda x: \" \".join([stemmer.stem(word) for word in str(x).lower().split()]))\n",
    "\n",
    "    \n",
    "    failTypeMaps = failTypeMaps.str.replace(\"[^\\w\\s]\", \" \")\n",
    "    failTypeMaps = failTypeMaps.str.replace(\"\\s+\", \" \")\n",
    "\n",
    "    # Kategori eşleştirmesi yap\n",
    "    category_map = {\n",
    "        \"sqljobfailure\": \"Bağlantı Hatası\",\n",
    "        \"mavi sap data transfer fail\": \"İşlem Hatası\",\n",
    "        \"mavidatatransfercheckstatus\": \"Durum Kontrol Hatası\",\n",
    "        \"mavi sap data transfer\": \"İşlem Hatası\",\n",
    "        \"mavi replenishment not started\": \"Başlamadı\",\n",
    "        \"mavi rpl failed\": \"İşlem Hatası\",\n",
    "        \"sqljobfailure-mavi sap data transfer\": \"Bağlantı Hatası\",\n",
    "        \"step12 loaddailycsv failed\": \"İşlem Hatası\",\n",
    "        \"mavi dt fail\": \"İşlem Hatası\",\n",
    "        \"dt failed\": \"İşlem Hatası\",\n",
    "    }\n",
    "    failTypeMaps = failTypeMaps.apply(lambda x: category_map.get(x, \"Diğer\"))\n",
    "\n",
    "    # Kategorik değerleri sayısal değerlere dönüştür\n",
    "    numeric_map = {\n",
    "        \"Bağlantı Hatası\": 1,\n",
    "        \"İşlem Hatası\": 2,\n",
    "        \"Durum Kontrol Hatası\": 3,\n",
    "        \"Başlamadı\": 4,\n",
    "        \"Diğer\": 5,\n",
    "    }\n",
    "    failTypeMaps = failTypeMaps.apply(lambda x: numeric_map[x])\n",
    "\n",
    "    return failTypeMaps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2954eadf-a104-456f-be0b-d2d808ef3184",
   "metadata": {},
   "outputs": [],
   "source": [
    "failTypeMaps_processed = text_process(failTypeMaps.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dec91f4-5d5e-4ae6-9800-88172b6d23b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "failTypeMaps_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b898f201-c03a-4291-8433-f85a12283538",
   "metadata": {},
   "outputs": [],
   "source": [
    "failTypeMaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbebc778-c40d-4577-98d8-87b1e91613d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "failSummaryMaps = df_sheet2['FailSummary']\n",
    "failSummaryMaps.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1718e3-f23d-4103-be25-ad9975c04a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "failSummaryMaps = failSummaryMaps.str.lower()\n",
    "failSummaryMaps = failSummaryMaps.replace('[^\\w\\s]', ' ')\n",
    "failSummaryMaps = failSummaryMaps.replace('\\s+', ' ')\n",
    "failSummaryMaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7a5c62-c5d4-4b5e-af20-f55a79d236fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "failSummaryCountsMaps = df_sheet2['FailSummary'].value_counts()\n",
    "\n",
    "# Her bir FailType'in adını ve sayısını içeren bir sözlük oluşturma\n",
    "failSummaryCountsMaps_dict = {failSummary: count for failSummary, count in failSummaryCountsMaps.items()}\n",
    "\n",
    "# Oluşturulan sözlüğü bir değişkene atama\n",
    "failSummaryCountsMaps_variable = failSummaryCountsMaps_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc9f38e-6274-4d46-bdce-4c264326d220",
   "metadata": {},
   "outputs": [],
   "source": [
    "failSummaryCountsMaps_variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d131da-99cf-427d-9e66-af94ec989003",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "def text_process_fail_summary(failSummaryMaps):\n",
    "    # Boş veya eksik değerleri temizle\n",
    "    failSummaryMaps = {k: v for k, v in failSummaryMaps.items() if v is not None}\n",
    "\n",
    "    # Kök bulma işlemi için stemmer oluştur\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    # Metin önişleme adımlarını uygula\n",
    "    processed_fail_summary = {stemmer.stem(word): count for word, count in failSummaryMaps.items()}\n",
    "\n",
    "    return processed_fail_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788cc45e-3260-4258-beb4-45886cbe7cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Eğer değer metin değilse, boş bir string olarak döndür\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    # Küçük harfe dönüştürme\n",
    "    text = text.lower()\n",
    "    # Noktalama işaretlerini kaldırma\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Sayıları kaldırma\n",
    "    text = ''.join([i for i in text if not i.isdigit()])\n",
    "    # Stop words'leri kaldırma\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "    # Kelimeleri lemmatize etme\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    text = ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "    return text\n",
    "\n",
    "# Verilerin önişlemden geçirilmesi\n",
    "preprocessed_data = {preprocess_text(key): value for key, value in failSummaryMaps.items()}\n",
    "print(preprocessed_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398999fe-0c7d-4a81-b896-1aa9c3c61913",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136109d9-18a9-4096-9054-c95f544766c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# FailSummary verileri\n",
    "failSummaryMaps = {\n",
    "    'Sales dosyası eksik': 4,\n",
    "    'Material dosyalarının birinde format bozukluğu bulunuyor.': 3,\n",
    "    'Season dosyası bozuk': 2,\n",
    "    'DT geç bitti': 2,\n",
    "    'tempde dolduğu için patlamış': 2,\n",
    "    'müşteri dosyalardaki verileri böldüğü için dosya sayısı artmış, disk dolmuş.': 1,\n",
    "     'SeasonCode smallint yapılmış. Artık sayı aşmaya başladı': 1,\n",
    " 'Disk dolduğu için hata aldı': 1,\n",
    " 'tempdb diskinde yer kalmamış': 1,\n",
    " 'müşteri CapacityGroup dosyasına yeni kolonlar eklemiş. ': 1,\n",
    " 'eksik dosya': 1,\n",
    " 'manuel ilerletildi': 1,\n",
    " 'müşteri eksik dosya göndermiş.': 1,\n",
    " 'Dosyaları eksik/geç koydukları için DT gec basladı': 1,\n",
    " 'Eksik dosya': 1,\n",
    " 'Müşteri dosya isim formatlarının hepsini değiştirmiş.': 1,\n",
    " 'Diskte yer kalmamış.': 1,\n",
    " 'Tempdb dolmuş': 1,\n",
    " 'DT Sales Eksik dosya': 1,\n",
    " 'Geliştirme hatası CampaignDiscount': 1,\n",
    " 'dt döngüye girmiş, bittiği halde tekrar tetiklenmiş bu da hatalara sebep oldu. ': 1,\n",
    " 'sunucuda yer kalmadığından dosyayı download edemedi': 1,\n",
    " 'dosyada hatalı satır vardı': 1,\n",
    " ' saptemp.stocklast tablosu debug ederken eski veride kaldı': 1,\n",
    " 'OptionSeasonHistory tablosunda season bilgisi tinyint verilmiş.': 1,\n",
    " 'etl.StagingProducts tablosuna duplike kayıt eklenmeye çalışılması hataya neden oldu.': 1,\n",
    " \"Maintenance runları db'yi kitlemiş \": 1,\n",
    " \"DT'de müşteri hatası sebebiyle normal\": 1,\n",
    " 'Sales dosyasında çift kayıt bulunuyordu.': 1,\n",
    " 'Müşteri malzeme ana verisine eklediğini söylediği 3 alanı (SHELFSTART, SHELFEND, PRCLASS ) eklemeden MaterialMaster dosyasını göndermiş. kolonlar eklenmeden gönderildiği için hata aldık': 1,\n",
    " 'Müşteri malzeme ana verisine eklediği 3 kolondan tipi int olan SHELFSTART SHELFEND alanlarına toplam 16 satır n/a değeri girdiği ve toplam 11 satır PRCLASS kolonuna uzun data gönderidği için hata aldık': 1,\n",
    " 'Gece alınan hatada saptemp.Product tablosunda güncellenen kolon sap.Product Tablosunda güncellenmediği için hata aldık': 1,\n",
    " 'Denizin bilgisai dahilindetest çalışması yapıldığı için hata almış': 1,\n",
    " 'CapacityGroupData_20230207.csv dosyasında hatalı kayıt geldi. Tablomuzda buna denk gelecek kolonun (OFFSEAKGRUP) veri tipi Date': 1,\n",
    " 'DT geç başlatılmasıyla ilişkili': 1,\n",
    " \"Kdate'den sonra yüklenen dosyalar mevcuttu. Bu yüzden bu dosyalar içeri alınamamıştı.\": 1,\n",
    " \"P diskinde yer kalmadığı ve s3'ten sync işlemi yapamadığı için fail alındı.\": 1,\n",
    " 'Sales datasında eksik veri vardı': 1,\n",
    " \"[MaviStaging].[sap].[TransferNonReplenishment] sp'sinde operation.Stores'da olmayan StoreCode'u Insert ederken hata aldık.\": 1,\n",
    " 'Müsteri 2 defa KDate yollamış ': 1,\n",
    " 'db dolmus': 1,\n",
    " 'Müşteri veri koyamadığı için normal': 1,\n",
    " 'Müşteri isteğiyle eklenen 4 kurala takıldı': 1,\n",
    " 'Azure network problemleri sebebiyle erişim problemleri yaşandı': 1,\n",
    " \"engine.AlgoRuns'ta Status 1 takılı kalmıs Run vardı\": 1,\n",
    " 'Run takılı kaldığı için tabloların güncel olmadığını gördük  ': 1,\n",
    " 'airflow version upgrade sonrası daglerin silinmesi': 1\n",
    "}\n",
    "\n",
    "# Veri ve etiketlerin ayrılması\n",
    "failSummaries = list(failSummaryMaps.keys())\n",
    "labels = list(failSummaryMaps.values())\n",
    "\n",
    "# TF-IDF vektörleştirici oluşturulması\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# SVM modeli oluşturulması\n",
    "svm_model = make_pipeline(StandardScaler(with_mean=False), SVC(kernel='linear'))\n",
    "\n",
    "# Modelin eğitilmesi\n",
    "X_train = vectorizer.fit_transform(failSummaries)\n",
    "svm_model.fit(X_train, labels)\n",
    "\n",
    "# Test için örnek bir failSummary\n",
    "test_failSummary = \"Müşteri dosyalardaki verileri böldüğü için dosya sayısı artmış, disk dolmuş.\"\n",
    "\n",
    "# Tahmin\n",
    "X_test = vectorizer.transform([test_failSummary])\n",
    "prediction = svm_model.predict(X_test)\n",
    "\n",
    "# Tahmin sonucu\n",
    "print(\"Tahmin edilen fail type:\", prediction[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d102f46-ad7d-4460-b1d2-c5d28cae61b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "solutionsMaps = df_sheet2['Solution']\n",
    "solutionsMaps.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7396882-a1a5-4aec-aa61-27746109a76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "solutionsMaps = solutionsMaps.str.lower()\n",
    "solutionsMaps = solutionsMaps.replace('[^\\w\\s]', ' ')\n",
    "solutionsMaps = solutionsMaps.replace('\\s+', ' ')\n",
    "solutionsMaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e772b449-914a-4593-b335-763056249d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "solutionCountMaps = df_sheet2['Solution'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456075dc-5f9f-4d4e-8b7d-3aad46977986",
   "metadata": {},
   "outputs": [],
   "source": [
    "solutionCountMaps_dict = {failSummary: count for failSummary, count in solutionCountMaps.items()}\n",
    "\n",
    "# Oluşturulan sözlüğü bir değişkene atama\n",
    "solutionCountMaps_variable = solutionCountMaps_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2232eca2-4634-4b23-bd4f-71184207231d",
   "metadata": {},
   "outputs": [],
   "source": [
    "solutionCountMaps_variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb49909e-6a15-4e98-a10b-9d2da9fd5357",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "with pd.ExcelFile(\"InventDatasetNew.xlsx\") as xls:\n",
    "    df_sheet3 = pd.read_excel(xls, sheet_name=\"Fails\")\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9262ea99-c69f-4ea7-b98d-83f6faf6711d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sheet3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0368f1-59f3-4c84-bb07-306685a5dbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Verilerin temizlenmesi\n",
    "df_sheet3['FailSummary'] = df_sheet3['FailSummary'].apply(preprocess_text)\n",
    "df_sheet3['FailType'] = df_sheet3['FailType'].apply(preprocess_text)\n",
    "df_sheet3['Customer'] = df_sheet3['Customer'].apply(preprocess_text)\n",
    "df_sheet3['DagName'] = df_sheet3['DagName'].apply(preprocess_text)\n",
    "df_sheet3['Source'] = df_sheet3['Source'].apply(preprocess_text)\n",
    "df_sheet3['SourceType'] = df_sheet3['SourceType'].apply(preprocess_text)\n",
    "df_sheet3['Solution'] = df_sheet3['Solution'].apply(preprocess_text)\n",
    "\n",
    "# Tüm sütunların string türüne dönüştürülmesi\n",
    "df_sheet3 = df_sheet3.astype(str)\n",
    "\n",
    "X = df_sheet3[['Customer','Source','SourceType', 'FailType', 'FailSummary','DagName']]\n",
    "y = df_sheet3['Solution']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "column_transformer = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('encoder', OneHotEncoder(handle_unknown='ignore'), ['Customer', 'FailType']),       \n",
    "        ('vectorizer', TfidfVectorizer(preprocessor=preprocess_text), 'FailSummary'),\n",
    "        ('vectorizer_sourceType', TfidfVectorizer(preprocessor=preprocess_text), 'SourceType'),\n",
    "        ('vectorizer_dag', TfidfVectorizer(preprocessor=preprocess_text),'DagName'),\n",
    "        ('vectorizer_source', TfidfVectorizer(preprocessor=preprocess_text), 'Source')\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "pipelineRocksTest = Pipeline([\n",
    "    ('preprocessor', column_transformer),\n",
    "    ('classifier', SVC(kernel='linear'))\n",
    "])\n",
    "\n",
    "pipelineRocksTest.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db20affe-d418-40cc-a059-74a840359a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = pipelineRocksTest.score(X_test, y_test)\n",
    "print(\"Model accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b534d2c3-e238-4fc4-89c9-3a493e0d7c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "df_sheet3 = pd.read_excel(\"InventTestDataset.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4724a850-f601-48ac-bc32-ed9c2d188a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.sparse import vstack\n",
    "import pandas as pd\n",
    "\n",
    "# Verilerin temizlenmesi (preprocess_text fonksiyonunun tanımlandığından emin olun)\n",
    "df_sheet1['FailSummary'] = df_sheet1['FailSummary'].apply(preprocess_text)\n",
    "df_sheet1['FailType'] = df_sheet1['FailType'].apply(preprocess_text)\n",
    "df_sheet1['Customer'] = df_sheet1['Customer'].apply(preprocess_text)\n",
    "df_sheet1['DagName'] = df_sheet1['DagName'].apply(preprocess_text)\n",
    "df_sheet1['Source'] = df_sheet1['Source'].apply(preprocess_text)\n",
    "df_sheet1['SourceType'] = df_sheet1['SourceType'].apply(preprocess_text)\n",
    "df_sheet1['Solution'] = df_sheet1['Solution'].apply(preprocess_text)\n",
    "\n",
    "df_sheet1['Source_Type'] = df_sheet1['SourceType'].fillna(df_sheet1['Source'])\n",
    "df_sheet1['Source_Type'] = df_sheet1['Source_Type'].apply(preprocess_text)\n",
    "\n",
    "df_sheet1.fillna(\"\", inplace=True)\n",
    "\n",
    "# Veri ve hedef sütunlarını ayarlama\n",
    "X = df_sheet1[['Customer', 'Source', 'SourceType', 'DagName', 'FailType', 'FailSummary']]\n",
    "y = df_sheet1['Solution']\n",
    "\n",
    "# Eğitim ve test setlerini oluşturma\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=442)\n",
    "\n",
    "# Kategorik sütunları seçme\n",
    "categorical_columns = ['Customer', 'Source', 'SourceType', 'DagName', 'FailType']\n",
    "\n",
    "# ColumnTransformer oluşturma\n",
    "column_transformer = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('vectorizer_customer', TfidfVectorizer(), 'Customer'),\n",
    "        ('onehot_encoder', OneHotEncoder(handle_unknown='ignore'), ['Source', 'Source_Type', 'DagName', 'FailType']),\n",
    "        ('vectorizer_failsummary', TfidfVectorizer(), 'FailSummary')  \n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# X_train ve X_test'i DataFrame olarak dönüştürme\n",
    "X_train_df = pd.DataFrame(X_train, columns=['Customer', 'Source', 'Source_Type', 'DagName', 'FailType', 'FailSummary'])\n",
    "X_test_df = pd.DataFrame(X_test, columns=['Customer', 'Source', 'Source_Type', 'DagName', 'FailType', 'FailSummary'])\n",
    "\n",
    "# X_train ve X_test'i dönüştürme\n",
    "X_train_transformed = column_transformer.fit_transform(X_train_df)\n",
    "X_test_transformed = column_transformer.transform(X_test_df)\n",
    "\n",
    "# Pipeline oluşturma\n",
    "pipeline_rocks = Pipeline([\n",
    "    ('clf', RandomForestClassifier())  # RandomForestClassifier veya başka bir sınıflandırıcı\n",
    "])\n",
    "\n",
    "# Modeli eğitme\n",
    "pipeline_rocks.fit(X_train_transformed, y_train)\n",
    "\n",
    "# Test seti üzerinde tahmin yapma\n",
    "y_pred = pipeline_rocks.predict(X_test_transformed)\n",
    "\n",
    "# Performansı değerlendirme (örneğin, accuracy_score kullanılabilir)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0de899-9fbb-407c-ad92-5b7b33ad7907",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sheet1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0e301c71-bbbe-4d09-862c-1d87ccfb5c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "61ba9068-f963-4a9a-864d-891678475373",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_sheet1['FailSummary'] = df_sheet1['FailSummary'].astype(\"string\")\n",
    "df_sheet1['FailType'] = df_sheet1['FailType'].astype(\"string\")\n",
    "df_sheet1['Customer'] = df_sheet1['Customer'].astype(\"string\")\n",
    "df_sheet1['DagName'] = df_sheet1['DagName'].astype(\"string\")\n",
    "df_sheet1['Source'] = df_sheet1['Source'].astype(\"string\")\n",
    "df_sheet1['SourceType'] = df_sheet1['SourceType'].astype(\"string\")\n",
    "df_sheet1['Solution'] = df_sheet1['Solution'].astype(\"string\")\n",
    "\n",
    "\n",
    "df_sheet1['FailSummary'] = df_sheet1['FailSummary'].apply(preprocess_text)\n",
    "df_sheet1['FailType'] = df_sheet1['FailType'].apply(preprocess_text)\n",
    "df_sheet1['Customer'] = df_sheet1['Customer'].apply(preprocess_text)\n",
    "df_sheet1['DagName'] = df_sheet1['DagName'].apply(preprocess_text)\n",
    "df_sheet1['Source'] = df_sheet1['Source'].apply(preprocess_text)\n",
    "df_sheet1['SourceType'] = df_sheet1['SourceType'].apply(preprocess_text)\n",
    "df_sheet1['Solution'] = df_sheet1['Solution'].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c79d38f6-75a7-483f-a61e-37442f16c92f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_encoded' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m X \u001b[38;5;241m=\u001b[39m df_sheet1[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCustomer\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFailType\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSource\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSourceType\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFailSummary\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDagName\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[0;32m      2\u001b[0m y \u001b[38;5;241m=\u001b[39m df_sheet1[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSolution\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m----> 5\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(X_encoded, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m342\u001b[39m)\n\u001b[0;32m      8\u001b[0m model \u001b[38;5;241m=\u001b[39m DecisionTreeClassifier()\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Modeli eğit\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_encoded' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "X = df_sheet1[['Customer', 'FailType', 'Source', 'SourceType', 'FailSummary','DagName']]\n",
    "y = df_sheet1['Solution']\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=342)\n",
    "\n",
    "\n",
    "model = DecisionTreeClassifier()\n",
    "\n",
    "# Modeli eğit\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Test setiyle modelin performansını değerlendir\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Model doğruluğu:\", accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638d32a8-38f6-4830-b6ef-99636b128670",
   "metadata": {},
   "outputs": [],
   "source": [
    "dot = export_graphviz(model)\n",
    "gorsel = graphviz.Source(dot)\n",
    "gorsel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abfd979-46e9-4ace-88f3-2c1cddcb57d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
